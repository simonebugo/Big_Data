{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonebugo/Big_Data/blob/main/MIL_LAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30b4d728",
      "metadata": {
        "id": "30b4d728"
      },
      "source": [
        "# Dowload the data from google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b6c8d9ca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "b6c8d9ca",
        "outputId": "2d4848da-ca84-4469-9f1e-1d4aa5b2b6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.20.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.10.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileURLRetrievalError",
          "evalue": "Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=12gAeqkWtmIM6HU7LSg3T_2t23vFx4sfm\n\nbut Gdown can't. Please check connections and permissions.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_url_from_gdrive_confirmation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mget_url_from_gdrive_confirmation\u001b[0;34m(contents)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Too many users have viewed or downloaded this file recently. Please try accessing the file again later. If the file you are trying to access is particularly large or is shared with many people, it may take up to 24 hours to be able to view or download the file. If you still can't access a file after 24 hours, contact your domain administrator.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1180613380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'https://drive.google.com/uc?id={FILE_ID}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'archive.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mgdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Extract it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume, format, user_agent, log_messages)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0murl_origin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             )\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFileURLRetrievalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0mfilename_from_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileURLRetrievalError\u001b[0m: Failed to retrieve file url:\n\n\tToo many users have viewed or downloaded this file recently. Please\n\ttry accessing the file again later. If the file you are trying to\n\taccess is particularly large or is shared with many people, it may\n\ttake up to 24 hours to be able to view or download the file. If you\n\tstill can't access a file after 24 hours, contact your domain\n\tadministrator.\n\nYou may still be able to access the file from the browser:\n\n\thttps://drive.google.com/uc?id=12gAeqkWtmIM6HU7LSg3T_2t23vFx4sfm\n\nbut Gdown can't. Please check connections and permissions."
          ]
        }
      ],
      "source": [
        "!pip install gdown\n",
        "import gdown\n",
        "FILE_ID = \"12gAeqkWtmIM6HU7LSg3T_2t23vFx4sfm\"\n",
        "url = f'https://drive.google.com/uc?id={FILE_ID}'\n",
        "output = 'archive.tar.gz'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Extract it\n",
        "import tarfile, os\n",
        "os.makedirs('data', exist_ok=True)\n",
        "with tarfile.open('archive.tar.gz', 'r:gz') as tar:\n",
        "    tar.extractall('data')\n",
        "\n",
        "print(\"File downloaded and extracted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "584db250",
      "metadata": {
        "id": "584db250"
      },
      "source": [
        "# Multiple Instance Learning (MIL) Lab\n",
        "\n",
        "In this lab, you'll implement a **Multiple Instance Learning (MIL)** pipeline in PyTorch.  \n",
        "Youâ€™ll work on:\n",
        "- Creating a dataset that groups instances into *bags*.\n",
        "- Implementing a **Attention** MIL model."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#occorre installare la libreria di openslide nel mio ambiente di lavoro prima di farne l'import (aggiunta da me siccome notebook non andava)\n",
        "!apt-get install openslide-tools\n",
        "!pip install openslide-python"
      ],
      "metadata": {
        "id": "C5NyjYBCe5An"
      },
      "id": "C5NyjYBCe5An",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b469f21",
      "metadata": {
        "id": "0b469f21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_curve, auc, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import openslide\n",
        "import h5py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bb67e2c",
      "metadata": {
        "id": "4bb67e2c"
      },
      "source": [
        "## âš™ï¸ Configuration\n",
        "\n",
        "This section defines global variables, file paths, and hyperparameters.  \n",
        "You donâ€™t need to modify this section â€” just review it to understand how the parameters control training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8020a148",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8020a148",
        "outputId": "ae94ce6c-b9e9-469f-b618-1215873abc34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# ---------------------- Config / Hyperparameters ----------------------\n",
        "DATA_DIR = \"./pt_files\"     # folder containing .pt files. directory in cui si trovano i file di dati di input, ovvero i file .pt che contengono gli embeddings pre-estratti.\n",
        "EMBED_DIM = 1024            # dimension of UNI embeddings. Ã¨ la dimensione delle feature (embedding) estratte da ciascuna patch dal modello UNI. in pratica UNI Ã¨ il modello utilizzato per estrarre le feature da ciascuna patch di una WSI\n",
        "PATCH_SIZE = 512           # size of each patch (in pixels). La dimensione (in pixel) della patch originale estratta dalla WSI (in questo caso 512 \\times 512 pixel)\n",
        "SEED = 42\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(DEVICE)\n",
        "\n",
        "NUM_EPOCHS = 2\n",
        "BATCH_SIZE = 8 #Il numero di borse (bags) che vengono elaborate contemporaneamente in ogni passaggio di addestramento\n",
        "HIDDEN_DIM = 256\n",
        "LR = 1e-4 #Ã¨ il learning rate\n",
        "WEIGHT_DECAY = 1e-5 #termine di regolarizzazione applicato all'ottimizzazione\n",
        "NUM_WORKERS = 4 #numero processi paralleli per cariare i dati da dsco al data loader\n",
        "\n",
        "\n",
        "# Reproducibility\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81e5030",
      "metadata": {
        "id": "b81e5030"
      },
      "source": [
        "## Implement the MIL Dataset\n",
        "\n",
        "In Multiple Instance Learning, data is organized into *bags*, each containing several *instances*.  \n",
        "Each bag has a label (positive or negative), but the individual instances do not.\n",
        "\n",
        "- Implement a custom PyTorch `Dataset` that loads instances and groups them into bags.\n",
        "\n",
        "- Each `__getitem__` call should return:\n",
        "  - `bag_tensor`: a tensor of shape `(num_instances, feature_dim)`\n",
        "  - `label`: an integer (0 or 1)\n",
        "  - `name`: the name of the sample\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49a91487",
      "metadata": {
        "id": "49a91487"
      },
      "outputs": [],
      "source": [
        "class EmbeddingBagDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (str): Directory containing features for each bag.\n",
        "        \"\"\"\n",
        "        # TODO:\n",
        "        # self.file_list = ...\n",
        "        # Trova tutti i file .pt\n",
        "        self.file_list = glob.glob(os.path.join(data_dir, '*.pt'))\n",
        "\n",
        "        # self.items = list of (file_path, label) tuples\n",
        "        # Estrae il nome base e l'etichetta per ogni file\n",
        "        self.items = []\n",
        "        for file_path in self.file_list:\n",
        "            filename = os.path.basename(file_path) #Estrae solo il nome del file dal percorso completo\n",
        "            label = infer_label_from_filename(filename) #Questa funzione legge la stringa filename e restituisce 0 se contiene 'normal' o 1 se contiene 'tumor'\n",
        "            self.items.append((file_path, label)) #Aggiunge una tupla alla lista self.items. La tupla contiene il percorso completo del file (file_path) e la sua etichetta binaria (label)\n",
        "\n",
        "        # self.labels = ...\n",
        "        # Estrae le etichette per lo split (usato fuori dalla classe)\n",
        "        self.labels = [label for (_path, label) in self.items]\n",
        "\n",
        "    def __len__(self):\n",
        "        # TODO: Return the total number of bags\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO:\n",
        "        # Load the bag data from the corresponding .pt file\n",
        "        file_path, label = self.items[idx]\n",
        "        #\n",
        "        # Returns:\n",
        "        #   bag_tensor: torch.FloatTensor\n",
        "        # Ogni file .pt contiene un tensore di forma (n_patches, 1024)]\n",
        "        bag_tensor = torch.load(file_path, map_location=torch.device('cpu')).float()\n",
        "        #   label: torch.LongTensor\n",
        "        # Assicurati che l'etichetta sia un tensore Long\n",
        "        label_tensor = torch.LongTensor([label])\n",
        "        #   name: str (filename without extension)\n",
        "        # Estrae il nome del file senza estensione (per la visualizzazione WSI)\n",
        "        name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "        return bag_tensor, label_tensor, name\n",
        "\n",
        "\n",
        "\n",
        "def infer_label_from_filename(fname):\n",
        "    # filenames normal_XXXX.pt are negative (0), tumor_XXXX.pt are positive (1)\n",
        "    if 'normal' in fname:\n",
        "        return 0\n",
        "    elif 'tumor' in fname:\n",
        "        return 1\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "spiegazione maschera: Quando il modello AB-MIL calcola l'attenzione, l'obiettivo Ã¨ trovare le patch piÃ¹ importanti (quelle tumorali)].\n",
        "Se calcoli l'attenzione su tutto il tensore paddato, il modello tratterÃ  le righe di zeri del padding come se fossero patch di tessuto reale.\n",
        "Questi zeri influenzerebbero (e distorcerebbero) in modo significativo il calcolo dell'attenzione e la media].\n",
        "La maschera agisce come un \"interruttore\" per dire al modello: \"Ignora questi elementi (gli zeri del padding)\"].\n",
        "La maschera Ã¨ un tensore booleano bidimensionale di forma (batch_size, max_n) costruito come segue:\n",
        "Valore True (1): Indica una posizione in cui Ã¨ presente una vera patch di dati]. L'attenzione deve essere calcolata qui.\n",
        "Valore False (0): Indica una posizione dove Ã¨ stato aggiunto lo zero padding]. L'attenzione non deve essere calcolata qui.\n",
        "\"\"\"\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch: list of tuples [(bag_tensor, label, name), ...]\n",
        "    Returns:\n",
        "        padded: torch.FloatTensor of shape (batch_size, max_n, emb_dim)\n",
        "        mask: torch.BoolTensor of shape (batch_size, max_n)\n",
        "        labels: torch.LongTensor of shape (batch_size,)\n",
        "        names: list of str of length batch_size\n",
        "    \"\"\"\n",
        "    # batch is a list of (bag_tensor, label, name) tuples\n",
        "    # the number of patches in each bag may vary and pytorch will not like it\n",
        "    # so we need to pad the bags to the same length\n",
        "    bags, labels, names = zip(*batch)\n",
        "    # TODO: figure out max number of patches per bag, and embedding dimension (should be EMBED_DIM, can be extracted from a bag)\n",
        "    max_n = max(b.shape[0] for b in bags)\n",
        "    emb_dim = bags[0].shape[1]  # Dovrebbe essere 1024, estraiamo dal primo elemento\n",
        "    padded = torch.zeros(len(bags), max_n, emb_dim) # Inizializza i tensori di output con zeri\n",
        "    mask = torch.zeros(len(bags), max_n, dtype=torch.bool) # Inizializza la maschera booleana a False (0) per il padding]\n",
        "    # TODO: into padded copy each bag, into mask put 1s where there is data (it will be used to ignore the padded parts when computing attention)\n",
        "    for i, b in enumerate(bags):\n",
        "        n_patches = b.shape[0]\n",
        "        # Copia il tensore originale nel tensore paddato\n",
        "        padded[i, :n_patches, :] = b\n",
        "        # Imposta la maschera a True (1) per tutti gli elementi che contengono dati reali\n",
        "        mask[i, :n_patches] = True\n",
        "    labels = torch.stack(labels).long()\n",
        "    return padded, mask, labels, list(names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18dbec33",
      "metadata": {
        "id": "18dbec33"
      },
      "outputs": [],
      "source": [
        "# Attention Based MIL --> https://arxiv.org/pdf/1802.04712\n",
        "# find the formula, be careful on the activation functions used\n",
        "\n",
        "\"\"\"\n",
        "la formula per la Gated Attention Ã¨:\n",
        "ð‘Ž=softmax(ð°^ð‘‡(tanh(ð•ð¡^ð‘‡ð‘˜) âŠ™ ðœŽ(ð”ð¡^ð‘‡ð‘˜)))\n",
        "dove  ð¡ð‘˜  Ã¨ l'embedding di una patch,\n",
        "ð•  e  ð”  sono le trasformazioni lineari,\n",
        "tanh  e  ðœŽ  (Sigmoid) sono le non-linearitÃ ,\n",
        "âŠ™  Ã¨ il prodotto elemento per elemento,\n",
        "e  ð°  proietta il risultato a 1 dimensione (il punteggio di attenzione grezzo).\n",
        "\"\"\"\n",
        "\n",
        "class GatedAttention(nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.V = nn.Linear(in_dim, hidden_dim)\n",
        "        self.U = nn.Linear(in_dim, hidden_dim)\n",
        "        self.w = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        Vx = torch.tanh(self.V(x))\n",
        "        Ux = torch.sigmoid(self.U(x))\n",
        "        H = Vx * Ux\n",
        "        scores = self.w(H)\n",
        "\n",
        "\n",
        "        if mask is not None:\n",
        "            #se una maschera Ã¨ fornita, imposta i punteggi degli elementi di padding a -inf\n",
        "            # La maschera deve essere invertita (~) perchÃ© True indica i dati, False il padding.\n",
        "            # Vogliamo che dove la maschera Ã¨ False, il punteggio sia -inf. in modo che non contribusicano come valori tramite il softmax in quanto = 0\n",
        "            scores = scores.masked_fill(~mask, float('-inf'))\n",
        "\n",
        "        attn = torch.softmax(scores, dim=1)\n",
        "        return attn\n",
        "\n",
        "\n",
        "class ABMIL(nn.Module):\n",
        "    def __init__(self, emb_dim, hidden_dim=256, n_classes=2, dropout=0.25):\n",
        "        super().__init__()\n",
        "        self.attention = GatedAttention(emb_dim, hidden_dim)\n",
        "        # TODO: define a classifier (MLP) that takes as input the bag representation and outputs logits for n_classes\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(emb_dim, emb_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(emb_dim, n_classes) # Output i logits per la CrossEntropyLoss\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn = self.attention(x, mask=mask)\n",
        "        bag = torch.sum(attn.unsqueeze(-1) * x, dim=1)\n",
        "        logits = self.classifier(bag)\n",
        "        return logits, attn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c0b2da",
      "metadata": {
        "id": "48c0b2da"
      },
      "source": [
        "### metrics, train and test epoch offered by our friend ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390d52ec",
      "metadata": {
        "id": "390d52ec"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred_probs, threshold=0.5):\n",
        "    y_pred = (y_pred_probs >= threshold).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    try:\n",
        "        roc = roc_auc_score(y_true, y_pred_probs)\n",
        "    except Exception:\n",
        "        roc = float('nan')\n",
        "    prec, recall, _ = precision_recall_curve(y_true, y_pred_probs)\n",
        "    aupr = auc(recall, prec)\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    return {\"accuracy\": acc, \"roc_auc\": roc, \"aupr\": aupr, \"confusion_matrix\": cm}\n",
        "\n",
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    losses = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    for x, mask, y, _names in tqdm(loader):\n",
        "        x, mask, y = x.to(device), mask.to(device), y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, _attn = model(x, mask=mask)\n",
        "        loss = criterion(logits, y)\n",
        "        probs = F.softmax(logits, dim=1)[:, 1].detach().cpu().numpy()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        losses.append(loss.item())\n",
        "        all_probs.extend(probs.tolist())\n",
        "        all_labels.extend(y.detach().cpu().numpy().tolist())\n",
        "    metrics = compute_metrics(np.array(all_labels), np.array(all_probs))\n",
        "    return np.mean(losses), metrics\n",
        "\n",
        "def eval_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    all_attn = []\n",
        "    names_list = []\n",
        "    with torch.no_grad():\n",
        "        for x, mask, y, names in tqdm(loader):\n",
        "            x, mask, y = x.to(device), mask.to(device), y.to(device)\n",
        "            logits, attn = model(x, mask=mask)\n",
        "            loss = criterion(logits, y)\n",
        "            probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "            losses.append(loss.item())\n",
        "            all_probs.extend(probs.tolist())\n",
        "            all_labels.extend(y.detach().cpu().numpy().tolist())\n",
        "            all_attn.extend(attn.cpu().numpy().tolist())\n",
        "            names_list.extend(names)\n",
        "    metrics = compute_metrics(np.array(all_labels), np.array(all_probs))\n",
        "    return np.mean(losses), metrics, all_attn, names_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96a98f5b",
      "metadata": {
        "id": "96a98f5b"
      },
      "outputs": [],
      "source": [
        "dataset = EmbeddingBagDataset(DATA_DIR)\n",
        "labels = np.array([lab for (_p, lab) in dataset.items])\n",
        "indices = np.arange(len(dataset))\n",
        "\n",
        "\n",
        "# split dataset into train, val, test\n",
        "train_idx, temp_idx = train_test_split(indices, test_size=0.30, stratify=labels, random_state=SEED)\n",
        "val_idx, test_idx = train_test_split(temp_idx, test_size=0.5, stratify=labels[temp_idx], random_state=SEED)\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
        "val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_idx)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Dataset sizes: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9476ee32",
      "metadata": {
        "id": "9476ee32"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Build model ----------------------\n",
        "model = ABMIL(emb_dim=EMBED_DIM, hidden_dim=HIDDEN_DIM, n_classes=2).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "287e2779",
      "metadata": {
        "id": "287e2779"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Training loop ----------------------\n",
        "best_val_auc = 0.0\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS}\")\n",
        "    train_loss, train_metrics = train_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
        "\n",
        "    val_loss, val_metrics, val_attn, val_names = eval_epoch(model, val_loader, criterion, DEVICE)\n",
        "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | Train loss: {train_loss:.4f} | Val loss: {val_loss:.4f} | Val AUROC: {val_metrics['roc_auc']:.4f}\")\n",
        "    if val_metrics[\"roc_auc\"] > best_val_auc:\n",
        "        best_val_auc = val_metrics[\"roc_auc\"]\n",
        "        torch.save({\"model_state\": model.state_dict(), \"EMBED_DIM\": EMBED_DIM}, \"abmil_best.pth\")\n",
        "        print(\"Saved best model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eb65b88",
      "metadata": {
        "id": "2eb65b88"
      },
      "outputs": [],
      "source": [
        "ckpt = torch.load(\"abmil_best.pth\", map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state\"])\n",
        "test_loss, test_metrics, test_attn, test_names = eval_epoch(model, test_loader, criterion, DEVICE)\n",
        "print(\"Test metrics:\", test_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a66ac8d1",
      "metadata": {
        "id": "a66ac8d1"
      },
      "source": [
        "# Plot the WSIs and show high attention patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d084fc5",
      "metadata": {
        "id": "8d084fc5"
      },
      "outputs": [],
      "source": [
        "def extract_samples_from_datasets(WSIs, datasets):\n",
        "    # figure out where the WSIs ended up (train/val/test)\n",
        "    # TODO: return a list of (bag_tensor, label, name) for each WSI found in any of the datasets\n",
        "    samples = []\n",
        "\n",
        "    # Prepara un set di nomi WSI senza estensione (ad esempio, 'normal_002', 'tumor_002')\n",
        "    wsi_names_to_find = {os.path.splitext(f)[0] for f in WSIs}\n",
        "\n",
        "    # Itera su tutti i subset (train, val, test)\n",
        "    for dataset_subset in datasets:\n",
        "        # Quando si itera su un torch.utils.data.Subset, si ottengono solo gli indici.\n",
        "        # Dobbiamo accedere all'oggetto dataset sottostante per recuperare i dati completi.\n",
        "\n",
        "        # 1. Ottiene il dataset sottostante (EmbeddingBagDataset)\n",
        "        # Se l'oggetto Ã¨ un Subset, estrai l'attributo 'dataset'\n",
        "        if isinstance(dataset_subset, torch.utils.data.Subset):\n",
        "            base_dataset = dataset_subset.dataset\n",
        "            indices = dataset_subset.indices\n",
        "        else:\n",
        "            base_dataset = dataset_subset\n",
        "            indices = range(len(base_dataset))\n",
        "\n",
        "        # 2. Itera sugli indici del subset e verifica se il nome corrisponde\n",
        "        for idx in indices:\n",
        "            # L'EmbeddingBagDataset (base_dataset) Ã¨ la fonte di self.items\n",
        "            file_path, label = base_dataset.items[idx]\n",
        "            name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "            if name in wsi_names_to_find:\n",
        "                # Carica la borsa completa (perchÃ© serve l'attention in seguito)\n",
        "                bag_tensor, label_tensor, name = base_dataset[idx]\n",
        "\n",
        "                # Rimuoviamo il nome per non cercarlo due volte (se necessario)\n",
        "                # wsi_names_to_find.remove(name)\n",
        "\n",
        "                samples.append((bag_tensor, label_tensor.item(), name)) # Convertiamo l'etichetta in int\n",
        "    return samples\n",
        "\n",
        "# Visualize attention for the WSIs\n",
        "WSIs = os.listdir('WSI')\n",
        "\n",
        "## NB, with random split the 2 patches for which we have WSI images may not be in test set, so we search in all subsets\n",
        "# You could also split the dataset to make sure these 2 are in test set, which would be cleaner (extra TODO if you are bored)\n",
        "samples = extract_samples_from_datasets(WSIs, [train_dataset, val_dataset, test_dataset])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e020f655",
      "metadata": {
        "id": "e020f655"
      },
      "source": [
        "## For each sample for which we have the WSI, extract the top patches with higher attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "617777fb",
      "metadata": {
        "id": "617777fb"
      },
      "outputs": [],
      "source": [
        "# ---------------------- Attention visualization ----------------------\n",
        "def top_k_instances(attn_weights, top_k=10):\n",
        "    # TODO: find out the indices of the top-k highest attention weights\n",
        "    # 1. Trova gli indici ordinati dei pesi (dal piÃ¹ piccolo al piÃ¹ grande)\n",
        "    sorted_indices = np.argsort(attn_weights)\n",
        "    # 2. Inverti l'ordine per avere gli indici dal piÃ¹ grande al piÃ¹ piccolo\n",
        "    top_indices_descending = sorted_indices[::-1]\n",
        "    # 3. Seleziona i primi top_k indici\n",
        "    top_k_instances = top_indices_descending[:top_k]\n",
        "    return top_k_instances\n",
        "\n",
        "\n",
        "model.eval()\n",
        "k = 5\n",
        "high_attention_patches = {}\n",
        "with torch.no_grad():\n",
        "    for sample_bag, sample_label, sample_name in samples:\n",
        "        x = sample_bag.unsqueeze(0).to(DEVICE)\n",
        "        mask = torch.ones(1, x.size(1), dtype=torch.bool).to(DEVICE)\n",
        "        logits, attn = model(x, mask=mask)\n",
        "        probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()\n",
        "        attn = attn.squeeze(0).cpu().numpy()\n",
        "        # get the top-k high attention patch indices\n",
        "        top_k = top_k_instances(attn, top_k=k)\n",
        "        high_attention_patches[sample_name] = top_k"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67658d3",
      "metadata": {
        "id": "d67658d3"
      },
      "source": [
        "## Figure out the coordinates of the high attention patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af674718",
      "metadata": {
        "id": "af674718"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "patch_dir = \"./patches/\"\n",
        "patches = os.listdir(patch_dir)\n",
        "\n",
        "for patch in patches:\n",
        "    patch_path = os.path.join(patch_dir, patch)\n",
        "    with h5py.File(patch_path, 'r') as f:\n",
        "        coords = f['coords'][:]\n",
        "    # coords is a numpy array of shape (num_patches, 2) with (x, y) coordinates of each patch\n",
        "    patch_name = os.path.basename(patch).split('.')[0]\n",
        "    top_k_coords = coords[high_attention_patches[patch_name]]\n",
        "    WSI = f\"./WSI/{patch_name}.tif\"\n",
        "    slide = openslide.OpenSlide(WSI)\n",
        "    level_0_dim = slide.level_dimensions[0]\n",
        "    print(f\"Level 0 dimensions: {level_0_dim}\")\n",
        "\n",
        "    # Create an empty mask of the size of level 0\n",
        "    mask = np.zeros((level_0_dim[1], level_0_dim[0]), dtype=np.uint8)\n",
        "    for coord in top_k_coords:\n",
        "        x_level0, y_level0 = coord\n",
        "        # TODO: set mask to 1 in the region corresponding to the patch (x_level0, y_level0) is the top-left corner of the patch of size PATCH_SIZE\n",
        "        # Imposta a 1 la regione corrispondente alla patch\n",
        "        # Usiamo np.clip per evitare che le coordinate vadano fuori dai limiti della maschera\n",
        "        x_end = np.clip(x_level0 + PATCH_SIZE, 0, level_0_dim[0])\n",
        "        y_end = np.clip(y_level0 + PATCH_SIZE, 0, level_0_dim[1])\n",
        "        mask[y_level0:y_end, x_level0:x_end] = 1\n",
        "\n",
        "    # Read WSI at a lower level for visualization otherwise if you try to plot it the image will be too large\n",
        "    x = 4\n",
        "    level_x = slide.read_region((0, 0), x, slide.level_dimensions[x])\n",
        "    # downsample the mask so that it matches level x (each level is downsampled by a factor of 2)\n",
        "    downsample_factor = 2**x\n",
        "\n",
        "    # Ridimensiona la maschera utilizzando solo la parte rilevante (i pixel a 1)\n",
        "    from skimage.transform import downscale_local_mean\n",
        "    # Esegue il downsampling della maschera\n",
        "    mask_downsample = downscale_local_mean(mask, (downsample_factor, downsample_factor)) > 0\n",
        "    mask_downsample = mask_downsample.astype(np.uint8)\n",
        "    print(f\"Mask downsampled shape: {mask_downsample.shape}, Level {x} dimensions: {slide.level_dimensions[x]}\")\n",
        "    # The image will still be too large and the patches will be small, only plot the part of the image where there are high attention patches\n",
        "    # TODO: find bounding box of the high attention patches in level x coordinates\n",
        "    # 1. Trova le coordinate minime e massime in Livello 0\n",
        "    min_x_l0 = top_k_coords[:, 0].min()\n",
        "    min_y_l0 = top_k_coords[:, 1].min()\n",
        "    max_x_l0 = top_k_coords[:, 0].max() + PATCH_SIZE\n",
        "    max_y_l0 = top_k_coords[:, 1].max() + PATCH_SIZE\n",
        "\n",
        "    # 2. Calcola il fattore di downsampling\n",
        "    downsample_factor = 2**x # lo usiamo qui (o slide.level_downsamples[x] se disponibile)\n",
        "\n",
        "    # 3. Trasforma le coordinate in Livello X\n",
        "    # Applichiamo un margine per vedere il contesto\n",
        "    margin = 50\n",
        "\n",
        "    xmin = np.clip((min_x_l0 // downsample_factor) - margin, 0, level_x.width)\n",
        "    ymin = np.clip((min_y_l0 // downsample_factor) - margin, 0, level_x.height)\n",
        "    xmax = np.clip((max_x_l0 // downsample_factor) + margin, 0, level_x.width)\n",
        "    ymax = np.clip((max_y_l0 // downsample_factor) + margin, 0, level_x.height)\n",
        "\n",
        "    # Assicurati che xmax > xmin e ymax > ymin per evitare errori di crop\n",
        "    if xmax <= xmin: xmax = xmin + 1\n",
        "    if ymax <= ymin: ymax = ymin + 1\n",
        "\n",
        "\n",
        "    level_x = level_x.crop((xmin, ymin, xmax, ymax))\n",
        "    mask_downsample = mask_downsample[ymin:ymax, xmin:xmax]\n",
        "    # display mask as red overlay on level x, with some transparency\n",
        "    plt.imshow(level_x)\n",
        "    plt.imshow(mask_downsample, cmap='Reds', alpha=0.5)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}